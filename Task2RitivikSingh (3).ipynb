{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Hindi train dataset from: C:\\Users\\madhuri\\Downloads\\Universal Dependencies 2.12\\ud-treebanks-v2.12\\ud-treebanks-v2.12\\UD_Hindi-HDTB\\hi_hdtb-ud-train.conllu\n",
      "Reading Hindi valid dataset from: C:\\Users\\madhuri\\Downloads\\Universal Dependencies 2.12\\ud-treebanks-v2.12\\ud-treebanks-v2.12\\UD_Hindi-HDTB\\hi_hdtb-ud-test.conllu\n",
      "Training the classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\madhuri\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]Making predictions on the valid set...\n",
      "Classifier Accuracy on the Valid Set: 0.9514817950889077\n",
      "Features for the input sentence: [{'word_यह': 1, 'right_word_एशिया': 1}, {'word_एशिया': 1, 'left_word_यह': 1, 'right_word_की': 1}, {'word_की': 1, 'left_word_एशिया': 1, 'right_word_सबसे': 1}, {'word_सबसे': 1, 'left_word_की': 1, 'right_word_बड़ी': 1}, {'word_बड़ी': 1, 'left_word_सबसे': 1, 'right_word_मस्जिदों': 1}, {'word_मस्जिदों': 1, 'left_word_बड़ी': 1, 'right_word_में': 1}, {'word_में': 1, 'left_word_मस्जिदों': 1, 'right_word_से': 1}, {'word_से': 1, 'left_word_में': 1, 'right_word_एक': 1}, {'word_एक': 1, 'left_word_से': 1, 'right_word_है': 1}, {'word_है': 1, 'left_word_एक': 1, 'right_word_।': 1}, {'word_।': 1, 'left_word_है': 1, 'right_word_इसे': 1}, {'word_इसे': 1, 'left_word_।': 1, 'right_word_नवाब': 1}, {'word_नवाब': 1, 'left_word_इसे': 1, 'right_word_शाहजेहन': 1}, {'word_शाहजेहन': 1, 'left_word_नवाब': 1, 'right_word_ने': 1}, {'word_ने': 1, 'left_word_शाहजेहन': 1, 'right_word_बनवाया': 1}, {'word_बनवाया': 1, 'left_word_ने': 1, 'right_word_था': 1}, {'word_था': 1, 'left_word_बनवाया': 1, 'right_word_।': 1}, {'word_।': 1, 'left_word_था': 1}]\n",
      "Predictions for the input sentence: ['DET' 'PROPN' 'ADP' 'ADV' 'ADJ' 'NOUN' 'ADP' 'ADP' 'NUM' 'AUX' 'PUNCT'\n",
      " 'PRON' 'NOUN' 'PROPN' 'ADP' 'VERB' 'AUX' 'PUNCT']\n",
      "End of script\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from conllu import parse\n",
    "import os\n",
    "\n",
    "def read_hindi_dataset(f_name):\n",
    "    with open(f_name, encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    sentences = parse(data)\n",
    "    return sentences\n",
    "\n",
    "def generate_sentence_features(sent):\n",
    "    sent_features = []\n",
    "    for word_idx, word_info in enumerate(sent):\n",
    "        word_features = {}\n",
    "        word_features[\"word_\" + word_info['form']] = 1\n",
    "        if word_idx != 0:\n",
    "            word_features[\"left_word_\" + sent[word_idx - 1]['form']] = 1\n",
    "        if word_idx != len(sent) - 1:\n",
    "            word_features[\"right_word_\" + sent[word_idx + 1]['form']] = 1\n",
    "        sent_features.append(word_features)\n",
    "    return sent_features\n",
    "\n",
    "def prep_data(sentences):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    for sentence in sentences:\n",
    "        sent_features = generate_sentence_features(sentence)\n",
    "        assert len(sent_features) == len(sentence)\n",
    "        all_features.extend(sent_features)\n",
    "        all_labels.extend([word_info['upostag'] for word_info in sentence])\n",
    "    return all_features, all_labels\n",
    "\n",
    "def test_sentence(classifier, vectorizer, sentence):\n",
    "    sentence_features = generate_sentence_features(sentence)\n",
    "    print(\"Features for the input sentence:\", sentence_features)\n",
    "    \n",
    "    X_sentence = vectorizer.transform(sentence_features)\n",
    "    predictions = classifier.predict(X_sentence)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "train_dataset_path = \"C:\\\\Users\\\\madhuri\\\\Downloads\\\\Universal Dependencies 2.12\\\\ud-treebanks-v2.12\\\\ud-treebanks-v2.12\\\\UD_Hindi-HDTB\\\\hi_hdtb-ud-train.conllu\"\n",
    "print(\"Reading Hindi train dataset from:\", train_dataset_path)\n",
    "\n",
    "\n",
    "if not os.path.exists(train_dataset_path):\n",
    "    raise FileNotFoundError(f\"File not found: {train_dataset_path}\")\n",
    "\n",
    "train_sentences_hindi = read_hindi_dataset(train_dataset_path)\n",
    "\n",
    "\n",
    "valid_dataset_path = \"C:\\\\Users\\\\madhuri\\\\Downloads\\\\Universal Dependencies 2.12\\\\ud-treebanks-v2.12\\\\ud-treebanks-v2.12\\\\UD_Hindi-HDTB\\\\hi_hdtb-ud-test.conllu\"\n",
    "print(\"Reading Hindi valid dataset from:\", valid_dataset_path)\n",
    "\n",
    "\n",
    "if not os.path.exists(valid_dataset_path):\n",
    "    raise FileNotFoundError(f\"File not found: {valid_dataset_path}\")\n",
    "\n",
    "valid_sentences_hindi = read_hindi_dataset(valid_dataset_path)\n",
    "\n",
    "\n",
    "train_features, train_labels = prep_data(train_sentences_hindi)\n",
    "valid_features, valid_labels = prep_data(valid_sentences_hindi)\n",
    "\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_features)\n",
    "X_valid = vectorizer.transform(valid_features)\n",
    "\n",
    "\n",
    "classifier = LinearSVC(C=1, verbose=1)\n",
    "print(\"Training the classifier...\")\n",
    "classifier.fit(X_train, train_labels)\n",
    "\n",
    "\n",
    "print(\"Making predictions on the valid set...\")\n",
    "valid_predictions = classifier.predict(X_valid)\n",
    "\n",
    "\n",
    "valid_accuracy = accuracy_score(valid_labels, valid_predictions)\n",
    "print(\"Classifier Accuracy on the Valid Set:\", valid_accuracy)\n",
    "\n",
    "\n",
    "user_input_sentence = \"यह एशिया की सबसे बड़ी मस्जिदों में से एक है । इसे नवाब शाहजेहन ने बनवाया था । \"\n",
    "user_input_tokens = user_input_sentence.split()\n",
    "\n",
    "\n",
    "dummy_sentence = [{'form': token} for token in user_input_tokens]\n",
    "\n",
    "\n",
    "user_input_predictions = test_sentence(classifier, vectorizer, dummy_sentence)\n",
    "print(\"Predictions for the input sentence:\", user_input_predictions)\n",
    "print(\"End of script\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
